---
layout: archive
title: "Portfolio"
permalink: /portfolio/
author_profile: true
---

<h1 style="font-size:1.8em; margin-bottom:25px;">Portfolio</h1>

<!-- Soft Robotic Snake -->
<div style="display:flex; align-items:center; justify-content:space-between; margin-bottom:40px; flex-wrap:wrap;">
  <div style="flex:1; min-width:250px; margin-right:20px;">
    <h2 style="margin:0; font-size:1.3em;">Soft Robotic Snake</h2>
    <em>University of Massachusetts Amherst · 2024 – Present</em>
      <p style="margin-top:10px;">
     Snakes are capable of traversing various substrates by employing distinct locomotor gaits, including lateral undulation, 
     concertina locomotion, and sidewinding. Applying this versatility to robotic systems holds potential for various applications, 
     including search and rescue operations, environmental surveying, and other field tasks. Earlier studies proposed that snakes rely
     on push points to generate forward propulsion (Gray, 1946); however, recent studies indicate that anisotropic friction created by ventral snake scales
     plays a more significant role in snake locomotion (Hu, 2009), which generate more friction in backward direction than in the forward direction. Therefore, the motivation of this study is to develop a snake-inspired soft robot capable of utilizing friction to generate locomotion. The design of the  vertebrae, 
    ribs and scales were inspired by biological snakes. McKibben pneumatic muscle actuators are also used to mimic snake muscles; iliocostalis and the semispinalis.
<hr>
    These project involves:
<ul>
  <li>Designing snake-inspired components in SolidWorks and fabricating prototypes using 3D printing.</li>
  <li>Elastomer molding to create and evaluate bioinspired scale structures for frictional anisotropy.</li>
  <li>Designing and executing locomotion experiments to assess performance on various surface conditions.</li>
  <li>Performing material testing to analyze bonding performance between elastomers and thermoplastics.</li>

</ul>     
     </p>
  </div>
  
</div>

<!-- Vertical image gallery for Soft Robotic Snake -->
<div style="display:flex; flex-direction:column; align-items:center; gap:25px; margin-bottom:40px;">
  <div style="text-align:center;">
    <img src="{{ '/images/snake1.png' | relative_url }}" alt="Snake Prototype Fabrication" style="width:300px; border-radius:10px;">
    <p style="margin-top:5px; font-size:0.95em;">Fabrication of soft robotic snake vertebrae using silicone molding.</p>
  </div>
  <div style="text-align:center;">
    <img src="{{ '/images/snake2.png' | relative_url }}" alt="Friction Testing" style="width:300px; border-radius:10px;">
    <p style="margin-top:5px; font-size:0.95em;">Experimental setup for friction and surface anisotropy measurements.</p>
  </div>
  <div style="text-align:center;">
    <img src="{{ '/images/snake3.png' | relative_url }}" alt="Locomotion Experiment" style="width:300px; border-radius:10px;">
    <p style="margin-top:5px; font-size:0.95em;">Locomotion tests on surfaces with varying frictional properties.</p>
  </div>
  <div style="text-align:center;">
    <img src="{{ '/images/snake4.png' | relative_url }}" alt="3D-Printed Components" style="width:300px; border-radius:10px;">
    <p style="margin-top:5px; font-size:0.95em;">3D-printed structural elements and skin mold prototypes.</p>
  </div>
</div>

  <div style="margin-top:25px; text-align:center;">
    <p style="margin-bottom:8px;"><strong>Locomotion Video</strong></p>
    <video width="700" controls>
      <source src="{{ '/images/snake_compressedVideo.mp4' | relative_url }}" type="video/mp4">
      Your browser does not support the video tag.
    </video>
  </div>

<hr>

<!-- Wrist-Worn Haptic Device -->
<div style="display:flex; align-items:center; justify-content:space-between; margin-bottom:40px; flex-wrap:wrap;">
  <div style="flex:1; min-width:250px; margin-right:20px;">
    <h2 style="margin:0; font-size:1.3em;">Wrist-Worn Haptic Device</h2>
    
    This project involved the design and implementation of a wrist-worn haptic interface capable of delivering tactile feedback through a custom voice-coil actuator. The system was developed as part of a senior thesis and integrates mechanical, electronic, and software subsystems into a compact wearable platform. Through extensive prototyping and user evaluation, the device demonstrated reliable actuation performance and was validated through user studies on stiffness discrimination, leading to two publications at IEEE World Haptics 2023.

<ul style="margin-top:10px;"> <li>Developed a <strong>custom voice-coil actuator</strong> using neodymium magnets and optimized coil geometry for enhanced force output.</li> <li>Designed and fabricated a <strong>wrist-worn mechanical case</strong> through iterative 3D printing (10+ prototypes) ensuring stability and ergonomics.</li> <li>Implemented <strong>embedded control firmware</strong> enabling real-time actuation and sensor feedback via <strong>Unity3D-based control interface</strong>.</li> <li>Conducted <strong>quantitative and user studies</strong> confirming the system’s ability to convey tactile cues with adjustable force profiles.</li> </ul>
  </div>
</div>

<!-- Vertical image gallery for Haptic Device -->
<div style="display:flex; flex-direction:column; align-items:center; gap:25px; margin-bottom:40px;">
  <div style="text-align:center;">
    <img src="{{ '/images/ww1.png' | relative_url }}" alt="Device Design" style="width:500px; border-radius:10px;">
    <p style="margin-top:5px; font-size:0.95em;">
     Demonstrates the actuator’s operation and force response under varying voltages, showing linear and stable performance.
     (From: DOI: 10.1109/LRA.2024.3360815)</p>
  </div>
  <div style="text-align:center;">
    <img src="{{ '/images/ww2.png' | relative_url }}" alt="Electronics Assembly" style="width:500px; border-radius:10px;">
    <p style="margin-top:5px; font-size:0.95em;">Shows user discrimination accuracy across actuator duty cycles, with higher sensitivity at moderate actuation levels. (From: DOI: 10.1109/LRA.2024.3360815)
</p>
  </div>
  <div style="text-align:center;">
    <img src="{{ '/images/ww3.png' | relative_url }}" alt="Testing Setup" style="width:500px; border-radius:10px;">
    <p style="margin-top:5px; font-size:0.95em;">Soft satin fabric ensures comfortable and stable contact while maintaining consistent force transmission during wear.</p>
  </div>
  <div style="text-align:center;">
    
    <!-- Two images side by side -->
<div style="display:flex; justify-content:center; align-items:flex-start; gap:20px; margin-bottom:10px;">
  <img src="{{ '/images/ww4.png' | relative_url }}" alt="Control Board" style="width:500px; border-radius:10px;">
  <img src="{{ '/images/ww5.png' | relative_url }}" alt="Additional Board View" style="width:500px; border-radius:10px;">
</div>
    <p style="margin-top:5px; font-size:0.95em;">Compact two-layer board integrating control, sensing, and actuation electronics for precise real-time performance.</p>
  </div>
</div>



<!-- Robotic Teleoperation System -->
<div style="margin-bottom:50px;">

  <h2 style="margin:0; font-size:1.3em;">Robotic Teleoperation System</h2>
  <em>Class Project · Low-cost dual-arm teleoperation platform using ROS and Arduino</em>

  <!-- Summary paragraph -->
  <p style="margin-top:10px;">
    This class project presents a low-cost robotic teleoperation system composed of a controller arm
    and a teleoperated arm, both designed and 3D-printed. Joint angles are measured using
    potentiometers and transmitted via ROS to replicate the controller arm’s motion on the teleoperated
    arm. The system demonstrates real-time position mapping and reliable pick-and-place tasks, with a
    measured communication delay of <strong>0.3–0.53 seconds</strong> determined through video analysis.
  </p>

  <!-- Bullet points -->
  <ul style="margin-top:10px;">
    <li>Designed and fabricated two kinematically similar robotic arms using SG90 servos and 3D-printed components.</li>
    <li>Used potentiometers as joint sensors, minimizing cost while maintaining accurate position feedback.</li>
    <li>Implemented a ROS-based publisher–subscriber framework for wireless bidirectional teleoperation.</li>
    <li>Validated real-time control through pick-and-place tasks with delay measurements derived from frame-by-frame video analysis.</li>
  </ul>

<!-- Image grid: 2 rows, 2 images per row with explanations -->
<div style="display:flex; flex-direction:column; align-items:center; gap:20px; margin-top:20px;">

  <!-- Row 1 -->
 <!-- Row with two images side by side -->
<div style="display:flex; justify-content:center; align-items:flex-start; gap:20px; margin-bottom:10px;">
  <img src="{{ '/images/controller1.png' | relative_url }}" alt="Control Board" style="width:500px; border-radius:10px;">
  <img src="{{ '/images/controller2.png' | relative_url }}" alt="Additional Board View" style="width:500px; border-radius:10px;">
</div>
  <p style="text-align:center; font-size:0.95em; max-width:1000px; margin-top:8px;">
    <strong>Controller Arm:</strong> 3D-printed design and assembly of the master arm used to control the teleoperated manipulator. 
    Potentiometers are integrated into each joint for angular position sensing and signal transmission through ROS.
  </p>

  <!-- Row 2 -->
  <!-- Row with two images side by side -->
<div style="display:flex; justify-content:center; align-items:flex-start; gap:20px; margin-bottom:10px;">
  <img src="{{ '/images/teleoperated1.png' | relative_url }}" alt="Control Board" style="width:500px; border-radius:10px;">
  <img src="{{ '/images/teleoperated2.png' | relative_url }}" alt="Additional Board View" style="width:400px; border-radius:10px;">
</div>
  <p style="text-align:center; font-size:0.95em; max-width:1000px; margin-top:8px;">
    <strong>Teleoperated Arm:</strong> Replica arm executing the controller’s movements in real time.
    Demonstrations include pick-and-place tasks with measured communication delays of 0.3–0.53 s determined via frame-by-frame video analysis.
  </p>

</div>


 <!-- Single image below -->
<div style="text-align:center; margin-top:20px;">
  <img src="{{ '/images/architecture.png' | relative_url }}" 
       alt="System Overview" 
       style="width:500px; max-width:100%; border-radius:10px;">
  <p style="text-align:center; font-size:0.95em; max-width:1000px; margin-top:8px;">
    <strong>System Architecture:</strong> Overview of the teleoperation framework showing the data flow between the 
    controller and teleoperated arms through the ROS publisher–subscriber architecture. Each joint angle is acquired 
    via potentiometers and transmitted to replicate movement on the teleoperated side. The setup enables low-cost, 
    real-time control with measured delays of 0.3–0.53 s under wireless operation.
  </p>
</div>


  <!-- Embedded video (local .mp4) -->
  <div style="margin-top:25px; text-align:center;">
    <p style="margin-bottom:8px;"><strong>Demonstration Video</strong></p>
    <video width="700" controls>
      <source src="{{ '/images/Demo4.mp4' | relative_url }}" type="video/mp4">
      Your browser does not support the video tag.
    </video>
  </div>

</div>




<hr>

<hr>

<!-- Haptic Game Development -->
<div style="display:flex; align-items:center; justify-content:space-between; margin-bottom:40px; flex-wrap:wrap;">
  <div style="flex:1; min-width:250px; margin-right:20px;">
    <h2 style="margin:0; font-size:1.3em;">Haptic Game Development</h2>
    <em>Kadir Has University · 2023</em>

    <p style="margin-top:10px;">
      This project introduced a <strong>smartphone-based endless runner game</strong> developed in 
      <strong>Unity (v.2022.3.11f1)</strong> to investigate how varying forms of 
      <strong>haptic feedback</strong> affect user interaction and engagement. 
      The objective was to build a <strong>gamified psychophysical testing platform</strong>, offering 
      a more engaging alternative to traditional tactile perception experiments. 
      By integrating <strong>vibration-based feedback</strong> through smartphone actuators, the game allows systematic 
      exploration of tactile response and sensory interaction within an enjoyable mobile gaming environment. 
      This work bridges the gap between <strong>human perception research</strong> and 
      <strong>interactive game design</strong>, demonstrating how smartphone haptics can serve both 
      scientific and entertainment purposes.
    </p>

    <ul style="margin-top:10px;">
      <li>Developed in <strong>Unity</strong> for <strong>iOS</strong> platforms, maintaining stable <strong>30 FPS performance</strong> with optimized haptic rendering.</li>
      <li>Designed an <strong>endless runner gameplay system</strong>, where players navigate paths and avoid obstacles using on-screen arrow controls.</li>
      <li>Implemented <strong>three haptic feedback modes</strong> with <strong>Interhaptics Composer</strong>, ranging from collision-based cues to predictive haptic guidance before wall contact.</li>
      <li>Employed <strong>procedural path generation</strong> with dynamically varying wall angles to ensure continuous, unpredictable gameplay for testing user adaptability.</li>
      <li>Enabled <strong>real-time data collection</strong> of user actions — collisions, inputs, and transitions between haptic regions — to support psychophysical analysis.</li>
      <li>Aimed to create a <strong>fun, accessible, and research-driven tool</strong> for studying multisensory interaction and human response under mobile gaming conditions.</li>
    </ul>
  </div>
</div>

<!-- Image placeholders -->
<div style="width:100%; text-align:center; margin-top:20px;">

  <div style="margin-bottom:20px;">
    <img src="{{ '/images/haptic_walls.png' | relative_url }}" 
         alt="Haptic Game Interface" 
         style="width:500px; max-width:100%; border-radius:10px;">
   <p style="font-size:0.9em; color:#555;">
  Screenshots of the mobile game during a playthrough. The player (a white ball) appears at the bottom of the screen, navigating a path bordered by brown walls. 
  Movement along the x-axis is controlled via on-screen arrow buttons, while progression along the y-axis occurs automatically. 
  The images show: (a) the start of the game, (b) a collision with the left wall, and (c) recovery back to the center path.
</p>

  </div>

  <div style="margin-bottom:20px;">
    <img src="{{ '/images/FeedbackLevel.png' | relative_url }}" 
         alt="Unity Game Environment" 
         style="width:500px; max-width:100%; border-radius:10px;">
<p style="font-size:0.9em; color:#555;">
  Representation of three haptic feedback modes: (i) fixed feedback triggered only on collision, 
  (ii) variable feedback intensity based on the ball’s penetration depth into the wall, and 
  (iii) predictive haptic guidance that increases in strength as the ball approaches and contacts the wall.
</p>  </div>

</div>


<hr>

<!-- Pod Design for Hyperloop Competition -->
<div style="display:flex; align-items:center; justify-content:space-between; margin-bottom:40px; flex-wrap:wrap;">
  <div style="flex:1; min-width:250px; margin-right:20px;">
    <h2 style="margin:0; font-size:1.3em;">Pod Design for Hyperloop Competition</h2>
    <em>National Hyperloop Competition · 2022</em>

    <p style="margin-top:10px;">
      This project was developed for a <strong>national Hyperloop competition</strong>, where our team received the 
      <strong>Best Technical Design Report Award</strong>. I worked as part of the 
      <strong>Navigation and Electronics Team</strong>, focusing on the design and testing of the systems that guided and monitored 
      the pod’s movement inside the tunnel. My main goal was to build a reliable and accurate navigation setup that could 
      work at high speeds using multiple sensors and Arduino-based control systems.
    </p>

    <ul style="margin-top:10px;">
      <li>Designed a <strong>reflector-based navigation system</strong> using <strong>Waveshare Laser Sensors</strong> and <strong>Arduino Due</strong> to detect tunnel reflectors at speeds up to <strong>26 m/s</strong>.</li>
      <li>Wrote and tested <strong>Arduino scripts</strong> to calculate velocity and displacement from reflector detection intervals.</li>
      <li>Helped design a <strong>redundant sensor layout</strong> with five laser modules to reduce error through majority voting logic.</li>
      <li>Developed a <strong>backup tachometer system</strong> using <strong>TCRT5000 IR sensors</strong> to track wheel rotation when laser sensors failed.</li>
      <li>Integrated <strong>MPU6050 IMU</strong> and <strong>height IR sensors</strong> to measure acceleration, tilt, and levitation height of the pod.</li>
      <li>Maintained signal accuracy by matching sampling rates to the <strong>Nyquist criterion</strong> for the laser module frequency.</li>
    </ul>
  </div>
</div>

<!-- Image placeholders -->
<div style="width:100%; text-align:center; margin-top:20px;">

  <div style="margin-bottom:20px;">
    <img src="{{ '/images/lasersensors.png' | relative_url }}" 
         alt="Pod Navigation Setup" 
         style="width:500px; max-width:100%; border-radius:10px;">
    <p style="font-size:0.9em; color:#555;">Navigation test setup showing reflector detection system and laser sensor alignment.</p>
  </div>

  <div style="margin-bottom:20px;">
    <img src="{{ '/images/navigation system.png' | relative_url }}" 
         alt="Sensor Integration" 
         style="width:500px; max-width:100%; border-radius:10px;">
    <p style="font-size:0.9em; color:#555;">Integration of IMU, laser modules, and IR sensors on the pod’s electronics chassis.</p>
  </div>

<div style="display:flex; justify-content:center; align-items:flex-start; gap:20px; margin-bottom:10px;">
  <img src="{{ '/images/realPod1.jpg' | relative_url }}" alt="Control Board" style="width:500px; border-radius:10px;">
  <img src="{{ '/images/realPod2.jpg' | relative_url }}" alt="Additional Board View" style="width:500px; border-radius:10px;">
</div>
  <p style="text-align:center; font-size:0.95em; max-width:1000px; margin-top:8px;">
    <strong>Teleoperated Arm:</strong> Replica arm executing the controller’s movements in real time.
    Demonstrations include pick-and-place tasks with measured communication delays of 0.3–0.53 s determined via frame-by-frame video analysis.
  </p>

</div>

</div>
